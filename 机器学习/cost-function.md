# Cost Function
We can measure the accuracy of our hypothesis function by using a **cost function**(代价函数). 

This takes an average difference (actually a fancier version of an average)
of all the results of the hypothesis with inputs from x's and the actual output y's.

$J\left(\theta_{0}, \theta_{1}\right)=\frac{1}{2 m} \sum_{i=1}^{m}\left(\hat{y}_{i}-y_{i}\right)^{2}=\frac{1}{2 m} \sum_{i=1}^{m}\left(h_{\theta}\left(x_{i}\right)-y_{i}\right)^{2}$

To break it apart, it is $\dfrac{1}{2} \bar x$
where $\bar{x}$  is the mean of the squares of $h_{\theta}\left(x_{i}\right)-y_{i}$, 
or the **difference** between the **predicted value** and the **actual value**.

This function is otherwise called the **"Squared error function"**(平方误差函数), or "**Mean squared error**". 

The mean is halved ${\dfrac{1}{2}}$ as a convenience for the computation of the gradient descent(梯度下降), 
as the derivative term (导数项) of the square function will cancel out the $\dfrac{1}{2}$ term. 

The following image summarizes what the cost function does:
![Cost Function](../img/Cost%20Function.png)
除以m是使得误差平均到每个样本，除以2是一个微积分技巧，用于消除计算偏导数时出现的2

If we try to think of it in visual terms, our training data set is scattered on the x-y plane. 
We are trying to make a straight line (defined by $h_\theta(x)$) which passes through these scattered data points(离散数据点).

Our objective is to get the best possible line. 
The best possible line will be such so that the **average squared vertical distances**(垂直距离的方均值) of the scattered points from the line will be the least. 


Ideally, the line should pass through all the points of our training data set. 
In such a case, the value of $J(\theta_0, \theta_1)$ will be 0. 


## $h_{\theta}(x) = \theta_1x$
The following example shows the ideal situation where we have a cost function of 0.
![cost function](../img/h(x)%20and%20j(θ).png)
$\theta_1 = 1$ 时对应损失函数为0

$\theta_1 = 0.5$ 时对应损失函数为0.58

将所有 $\theta_1$的值算出，连线

Thus as a goal, we should try to minimize the cost function. 

In this case, $\theta_1 = 1$ is our global minimum.

## $h_{\theta}(x) = \theta_0 + \theta_1x$
A **contour plot**(轮廓图，等高线图) is a graph that contains many contour lines. 

A contour line of a two variable function has a constant value at all points of the same line. 

An example of such a graph is the one to the right below.

![contour plot](../img/contour%20plot.png)

Taking any color and going along the 'circle', one would expect to get the same value of the cost function. 

For example, the three green points found on the green line above have the same value for $J(\theta_0,\theta_1)$ and 
as a result, they are found along the same line. 

The circled x displays the value of the cost function for the graph on the left when $\theta_0 = 800$ and $\theta_1 = -0.15$. 

Now giving our hypothesis function a slightly positive slope results in a better fit of the data.

![minimizing](../img/minimizes%20the%20cost%20function.png)

The graph above minimizes the cost function as much as possible and consequently, 
the result of $\theta_1$ and $\theta_0$ tend to be around 0.12 and 250 respectively. 

Plotting those values on our graph to the right seems to put our point in the center of the inner most 'circle'.

计算代价函数 $j(\theta)$ 的算法：
- [[gradient-descent]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[gradient-descent]: gradient-descent "Gradient Descent"
[//end]: # "Autogenerated link references"