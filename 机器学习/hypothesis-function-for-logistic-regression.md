# Hypothesis Function for Logistic Regression

## Hypothesis Function

We could approach the classification problem ignoring the fact that y is discrete-valued, 
and use our old linear regression algorithm to try to predict y given x. 

However, it is easy to construct examples where this method performs very poorly. 

Intuitively(直观地), it also doesn’t make sense for $h_\theta (x)$ to take values larger than 1 or smaller than 0 
when we know that y ∈ {0, 1}. 

To fix this, let’s change the form for our hypotheses $h_\theta (x)$ to satisfy $0 \leq h_\theta (x) \leq 1$. 

This is accomplished by plugging $\theta^Tx$ into the Logistic Function.

Our new form uses the "**Sigmoid Function**", also called the "**Logistic Function**":

$h_{\theta}(x)=g\left(\theta^{T} x\right)$

$z=\theta^{T} x$

$g(z)=\frac{1}{1+e^{-z}}$

The following image shows us what the sigmoid function looks like:

![sigmoid function](../img/Logistic%20function.png)

The function g(z), shown here, maps any real number to the (0, 1) interval,
> 此处显示的函数g（z）将任何实数映射到（0，1）区间


making it useful for transforming an arbitrary-valued function into a function better suited for classification.

**$h_\theta(x)$ will give us the probability that our output is 1.**

For example, $h_\theta(x)=0.7$ gives us a probability of 70% that our output is 1. 

Our probability that our prediction is 0 is just the complement of our probability that it is 1 
(e.g. if probability that it is 1 is 70%, then the probability that it is 0 is 30%).

$h_{\theta}(x)=P(y=1 \mid x ; \theta)=1-P(y=0 \mid x ; \theta)$

$P(y=0 \mid x ; \theta)+P(y=1 \mid x ; \theta)=1$

---

## Decision Boundary

In order to get our discrete 0 or 1 classification, 
we can translate the output of the hypothesis function as follows:

$h_{\theta}(x) \geq 0.5 \rightarrow y=1$

$h_{\theta}(x)<0.5 \rightarrow y=0$

The way our logistic function g behaves is that when its input is greater than or equal to zero, 
its output is greater than or equal to 0.5:

$g(z) \geq 0.5 \quad$ when $z \geq 0$

> $z=0, e^{0}=1 \Rightarrow g(z)=1 / 2$
> 
> $z \rightarrow \infty, e^{-\infty} \rightarrow 0 \Rightarrow g(z)=1$
> 
> $z \rightarrow-\infty, e^{\infty} \rightarrow \infty \Rightarrow g(z)=0$

So if our input to g is $\theta^T X$, then that means:

$h_{\theta}(x)=g\left(\theta^{T} x\right) \geq 0.5$  when  $\theta^{T} x \geq 0$

From these statements we can now say:

$\theta^{T} x \geq 0 \Rightarrow y=1$

$\theta^{T} x<0 \Rightarrow y=0$

![Decision Boundary](../img/Decision%20Boundary.png)

---


- back to [[hypothesis-function]]
- back to [[logistic-regression]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[hypothesis-function]: hypothesis-function "Hypothesis Function"
[logistic-regression]: logistic-regression "Logistic Regression"
[//end]: # "Autogenerated link references"