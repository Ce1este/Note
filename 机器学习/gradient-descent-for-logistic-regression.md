# Gradient Descent for Logistic Regression

Remember that the general form of gradient descent is:

>Repeat \{
>
>$\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial \theta_{j}} J(\theta)$
>
>}

We can work out the derivative part using calculus to get:

>Repeat \{ 
>
>$\theta_{j}:=\theta_{j}-\frac{\alpha}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) x_{j}^{(i)}$
>
>}

Notice that this algorithm is identical to(等同于) the one we used in linear regression. 
We still have to simultaneously update all values in theta.

A vectorized implementation is:

$\theta:=\theta-\frac{\alpha}{m} X^{T}(g(X \theta)-\vec{y})$

![Derivation of the formula](../img/Derivation%20of%20the%20formula.png)
---

- back to [[logistic-regression]] 
- back to [[gradient-descent]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[logistic-regression]: logistic-regression "Logistic Regression"
[gradient-descent]: gradient-descent "Gradient Descent"
[//end]: # "Autogenerated link references"