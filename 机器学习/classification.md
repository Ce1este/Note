# Classification

## Binary Classification

To attempt classification, 
one method is to use **linear regression** and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. 

However, this method doesn't work well because **classification is not actually a linear function**.

The classification problem is just like the regression problem, 
except that the values we now want to predict take on only a small number of discrete values. 

For now, we will focus on the **binary classification problem** in which y can take on only two values,
0 and 1. 

(Most of what we say here will also generalize to(推广到) the multiple-class case.) 

For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, 
and y may be 1 if it is a piece of spam mail, and 0 otherwise. 

Hence(因此), y∈{0,1}. 0 is also called the **negative class**, and 1 the **positive class**, 
and they are sometimes also denoted by the symbols “-” and “+.” 

Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example.

---

## Multiclass Classification: One-vs-all

Now we will approach the classification of data when we have **more than two categories**. 

Instead of y = {0,1} we will expand our definition so that y = {0,1...n}.

Since y = {0,1...n}, 
we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; 

in each one, we predict the probability that 'y' is a member of one of our classes.


$y \in\{0,1 \ldots n\}$

$h_{\theta}^{(0)}(x)=P(y=0 \mid x ; \theta)$

$h_{\theta}^{(1)}(x)=P(y=1 \mid x ; \theta)$

$\cdots$

$h_{\theta}^{(n)}(x)=P(y=n \mid x ; \theta)$

prediction $=\max _{i}\left(h_{\theta}^{(i)}(x)\right)$

We are basically **choosing one class** and then **lumping(总集) all the others into a single second class**. 

We do this repeatedly, applying binary logistic regression to each case, 
and then use the hypothesis that **returned the highest value as our prediction**.

The following image shows how one could classify 3 classes:

![One-vs-all](../img/One-vs-all.png)

---

## Neural Networks: Multiclass Classification


To classify data into multiple classes, we let our hypothesis function return a vector of values. 

Say we wanted to classify our data into one of four categories. 

We will use the following example to see how this classification is done. 

This algorithm takes as input an image and classifies it accordingly:

![multiple output units](../img/Multiple%20output%20units.png)

We can define our set of resulting classes as y:

$y^{(i)}=\left[\begin{array}{l}1 \\ 0 \\ 0 \\ 0\end{array}\right],\left[\begin{array}{l}0 \\ 1 \\ 0 \\ 0\end{array}\right],\left[\begin{array}{l}0 \\ 0 \\ 1 \\ 0\end{array}\right],\left[\begin{array}{l}0 \\ 0 \\ 0 \\ 1\end{array}\right]$

Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. 

The inner layers, each provide us with some new information which leads to our final hypothesis function. 

The setup looks like:

$\left[\begin{array}{c}x_{0} \\ x_{1} \\ x_{2} \\ \cdots \\ x_{n}\end{array}\right] \rightarrow\left[\begin{array}{c}a_{0}^{(2)} \\ a_{1}^{(2)} \\ a_{2}^{(2)} \\ \cdots\end{array}\right] \rightarrow\left[\begin{array}{c}a_{0}^{(3)} \\ a_{1}^{(3)} \\ a_{2}^{(3)} \\ \cdots\end{array}\right] \rightarrow \cdots \rightarrow\left[\begin{array}{c}h_{\Theta}(x)_{1} \\ h_{\Theta}(x)_{2} \\ h_{\Theta}(x)_{3} \\ h_{\Theta}(x)_{4}\end{array}\right]$

Our resulting hypothesis for one set of inputs may look like:

$h_{\Theta}(x)=\left[\begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array}\right]$


In which case our resulting class is the third one down, or $h_\Theta(x)_3$, 
which represents the motorcycle.


---


- math model for classification: [[logistic-regression]]
- back to [[supervised-learning]]
- back to [[neural-networks]]



[//begin]: # "Autogenerated link references for markdown compatibility"
[logistic-regression]: logistic-regression "Logistic Regression"
[supervised-learning]: supervised-learning "Supervised Learning"
[neural-networks]: neural-networks "Neural Networks"
[//end]: # "Autogenerated link references"