# Gradient Descent For Linear Regression

## Linear Regression with one variable

When specifically applied to the case of linear regression, 
a new form of the gradient descent equation can be derived. 

We can substitute our actual cost function and our actual hypothesis function and modify the equation to :

![a new form of the gradient descent equation](../img/a%20new%20form%20of%20the%20gradient%20descent%20equation.png)

-  **m** is **the size of the training set**
-  $\theta_0$ a constant that will be changing simultaneously with $\theta_1$
-  $x^{(i)}, y^{(i)}$ are values of the given training set (data)

Note that we have separated out the two cases for $\theta_j$ into separate equations for $\theta_0$and $\theta_1$; 

and that for $\theta_1$ we are multiplying $x_{i}$ at the end due to the derivative. 

The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.

---

So, this is simply gradient descent on the original cost function J. 

This method looks at every example in the entire training set on every step, 
and is called **batch gradient descent**. 

> “Batch”:Each step of gradient descent uses all the training examples.

Note that, while **gradient descent can be susceptible to local minima**(易受局部最小值影响) in general.

The optimization(优化) problem we have posed here for linear regression has only one global, and no other local, optima; 
thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. 

Indeed, J is a convex quadratic function(凸二次函数). 

Here is an example of gradient descent as it is run to minimize a quadratic function.

![a quadratic function](../img/gradient%20descent%20-%20a%20quadratic%20function.png)

The ellipses(椭圆) shown above are the contours(等高线) of a quadratic function. 

Also shown is the trajectory(轨迹) taken by gradient descent, 
which was initialized at (48,30). 

The x’s in the figure (joined by straight lines) mark the successive values of θ that 
gradient descent went through as it converged to its minimum.

---

## Linear Regression with Multiple Variables
The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:

repeat until convergence:{

$\theta_{0}:=\theta_{0}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{0}^{(i)}$

$\theta_{1}:=\theta_{1}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{1}^{(i)}$

$\theta_{2}:=\theta_{2}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{2}^{(i)}$

$\cdots$

}

In other words:

repeat until convergence:{

$\theta_{j}:=\theta_{j}-\alpha \frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)} \quad$ for $j:=0 . . \mathrm{n}$

}

### Matrix Notation

The Gradient Descent rule can be expressed as:

$\theta := \theta - \alpha \nabla J(\theta)$

Where $\nabla J(\theta)$ is a column vector of the form:

$\nabla J(\theta)=\left[\dfrac{\partial J(\theta)}{\partial \theta_{0}} \quad \dfrac{\partial J(\theta)}{\partial \theta_{1}} \quad \cdots \quad \dfrac{\partial J(\theta)}{\partial \theta_{n}}\right]$

The j-th component of the gradient is the summation of the product of two terms:

$\begin{aligned} \frac{\partial J(\theta)}{\partial \theta_{j}} &=\frac{1}{m} \sum_{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \cdot x_{j}^{(i)} \\ &=\frac{1}{m} \sum_{i=1}^{m} x_{j}^{(i)} \cdot\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right) \end{aligned}$

Sometimes, the summation of the product of two terms can be expressed as the product of two vectors.

Here, $x_j^{(i)}$ , for i = 1,...,m, represents the m elements of the j-th column, 
$\vec{x_j}$ , of the training set X.

The other term $(\left(h_\theta(x^{(i)}) - y^{(i)} \right)$ is the vector of the deviations 
between the predictions $h_\theta(x^{(i)})$h and the true values $y^{(i)}$. 

Re-writing $\frac{\partial J(\theta)}{\partial \theta_j}$ , we have:

$\dfrac{\partial J(\theta)}{\partial \theta_{j}}=\dfrac{1}{m} \overrightarrow{x_{j}}^{T}(X \theta-\vec{y})$

$\nabla J(\theta) \quad=\dfrac{1}{m} X^{T}(X \theta-\vec{y})$

Finally, the matrix notation (vectorized) of the Gradient Descent rule is:

$\theta:=\theta-\dfrac{\alpha}{m} X^{T}(X \theta-\vec{y})$

---



- back to [[gradient-descent]]
- back to [[linear-regression]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[gradient-descent]: gradient-descent "Gradient Descent"
[linear-regression]: linear-regression "Linear Regression"
[//end]: # "Autogenerated link references"