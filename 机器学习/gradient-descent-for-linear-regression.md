# Gradient Descent For Linear Regression

When specifically applied to the case of linear regression, 
a new form of the gradient descent equation can be derived. 

We can substitute our actual cost function and our actual hypothesis function and modify the equation to :

![a new form of the gradient descent equation](../img/a%20new%20form%20of%20the%20gradient%20descent%20equation.png)

-  **m** is **the size of the training set**
-  $\theta_0$ a constant that will be changing simultaneously with $\theta_1$
-  $x^{(i)}, y^{(i)}$ are values of the given training set (data)

Note that we have separated out the two cases for $\theta_j$ into separate equations for $\theta_0$and $\theta_1$; 

and that for $\theta_1$ we are multiplying $x_{i}$ at the end due to the derivative. 

The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate.

---

So, this is simply gradient descent on the original cost function J. 

This method looks at every example in the entire training set on every step, 
and is called **batch gradient descent**. 

> “Batch”:Each step of gradient descent uses all the training examples.

Note that, while **gradient descent can be susceptible to local minima**(易受局部最小值影响) in general.

The optimization(优化) problem we have posed here for linear regression has only one global, and no other local, optima; 
thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. 

Indeed, J is a convex quadratic function(凸二次函数). 

Here is an example of gradient descent as it is run to minimize a quadratic function.

![a quadratic function](../img/gradient%20descent%20-%20a%20quadratic%20function.png)

The ellipses(椭圆) shown above are the contours(等高线) of a quadratic function. 

Also shown is the trajectory(轨迹) taken by gradient descent, 
which was initialized at (48,30). 

The x’s in the figure (joined by straight lines) mark the successive values of θ that 
gradient descent went through as it converged to its minimum.

- back to [[gradient-descent]]
- back to [[linear-regression]]


[//begin]: # "Autogenerated link references for markdown compatibility"
[gradient-descent]: gradient-descent "Gradient Descent"
[linear-regression]: linear-regression "Linear Regression"
[//end]: # "Autogenerated link references"