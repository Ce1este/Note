# Regularized Logistic Regression

We can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. 

The following image shows how the regularized function, 
displayed by the pink line, 
is less likely to overfit than the non-regularized function represented by the blue line:

![Regularized Logistic Regression](../img/Regularized%20Logistic%20Regression.png)

## Cost Function

Recall that our cost function for logistic regression was:

$J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]$

We can regularize this equation by adding a term to the end:

$J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left[y^{(i)} \log \left(h_{\theta}\left(x^{(i)}\right)\right)+\left(1-y^{(i)}\right) \log \left(1-h_{\theta}\left(x^{(i)}\right)\right)\right]+\frac{\lambda}{2 m} \sum_{j=1}^{n} \theta_{j}^{2}$

The second sum, $\sum_{j=1}^n \theta_j^2$ means to explicitly exclude the bias term, 
$\theta_0$. 

I.e. the Î¸ vector is indexed from 0 to n (holding n+1 values, $\theta_0$ through $\theta_n$), 
and this sum explicitly skips $\theta_0$, by running from 1 to n, skipping 0. 

Thus, when computing the equation, we should continuously update the two following equations:

![Regularized Logistic Regression](../img/gradient%20descent_Regularized%20Logistic%20Regression.png)


-back to [[logistic-regression]]



[//begin]: # "Autogenerated link references for markdown compatibility"
[logistic-regression]: logistic-regression "Logistic Regression"
[//end]: # "Autogenerated link references"